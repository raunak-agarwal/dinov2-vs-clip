{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6a2193b-be20-4589-8576-1adc6c85535c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__pycache__\t\t\t    image-text-retrieval-evals.ipynb\n",
      "attention_pool.py\t\t    improved_dataloaders.py\n",
      "attention_pool_helpers.py\t    losses.py\n",
      "chexpert\t\t\t    metrics.py\n",
      "chexpert.zip\t\t\t    mimic\n",
      "clip_training_config-mimic_lt.yaml  mimic.zip\n",
      "clip_training_config-nih.yaml\t    ml_decoder.py\n",
      "cxr-bert-custom-vit.json\t    modeling.py\n",
      "cxr-bert-epoch_50.pt\t\t    output.log\n",
      "dino-mld-bce-1e-5\t\t    siglip-cxr-bert-custom-vit.json\n",
      "dino_model_config.yaml\t\t    siglip-cxr-bert-epoch_50.pt\n",
      "dino_training_config-mimic_lt.yaml  training_99999.pth\n",
      "dino_training_config-nih.yaml\t    wandb\n",
      "dinov2\t\t\t\t    wget-log\n",
      "finetune.py\t\t\t    wget-log.1\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53dd9aac-498c-488d-aefb-0b076c67e7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from open_clip import create_model_and_transforms, get_tokenizer\n",
    "from open_clip.factory import _MODEL_CONFIGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "729d801a-c0e7-490e-bab9-1fa2b380ccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and config files\n",
    "model_name = \"cxrclip_local\"\n",
    "\n",
    "with open(\"siglip-cxr-bert-custom-vit.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "    model_cfg = config[\"model_cfg\"]\n",
    "    preprocess_cfg = config[\"preprocess_cfg\"]\n",
    "\n",
    "_MODEL_CONFIGS[model_name] = model_cfg\n",
    "tokenizer = get_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b862e6e9-b0b7-4587-a2e9-801eae9554ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomTextCLIP(\n",
       "  (visual): TimmModel(\n",
       "    (trunk): VisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (patch_drop): Identity()\n",
       "      (norm_pre): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): DropPath(drop_prob=0.009)\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): DropPath(drop_prob=0.009)\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): DropPath(drop_prob=0.018)\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): DropPath(drop_prob=0.018)\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): DropPath(drop_prob=0.027)\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): DropPath(drop_prob=0.027)\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): DropPath(drop_prob=0.036)\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): DropPath(drop_prob=0.036)\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): DropPath(drop_prob=0.045)\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): DropPath(drop_prob=0.045)\n",
       "        )\n",
       "        (6): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): DropPath(drop_prob=0.055)\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): DropPath(drop_prob=0.055)\n",
       "        )\n",
       "        (7): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): DropPath(drop_prob=0.064)\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): DropPath(drop_prob=0.064)\n",
       "        )\n",
       "        (8): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): DropPath(drop_prob=0.073)\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): DropPath(drop_prob=0.073)\n",
       "        )\n",
       "        (9): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): DropPath(drop_prob=0.082)\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): DropPath(drop_prob=0.082)\n",
       "        )\n",
       "        (10): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): DropPath(drop_prob=0.091)\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): DropPath(drop_prob=0.091)\n",
       "        )\n",
       "        (11): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): DropPath(drop_prob=0.100)\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): DropPath(drop_prob=0.100)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (fc_norm): Identity()\n",
       "      (head_drop): Dropout(p=0.0, inplace=False)\n",
       "      (head): Identity()\n",
       "    )\n",
       "    (head): Sequential(\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (text): HFTextEncoder(\n",
       "    (transformer): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): MeanPooler()\n",
       "    (proj): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=768, bias=False)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=768, out_features=768, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in case of siglip, we set init_logit_bias=-10 in the config. in case of clip, remove it entirely\n",
    "model, _, preprocess = create_model_and_transforms(\n",
    "    model_name=model_name,\n",
    "    pretrained=\"siglip-cxr-bert-epoch_50.pt\",\n",
    "    **{f\"image_{k}\": v for k, v in preprocess_cfg.items()},\n",
    ")\n",
    "\n",
    "# Process the tar file\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "039b7966-9d87-4a65-b283-e0568e3fb2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files\t\t\t\t       mimic-cxr-lt-merged.csv\n",
      "mimic-cxr-2.0.0-merged-with-paths.csv  mimic_cxr_labels_reports_splits.csv\n"
     ]
    }
   ],
   "source": [
    "!ls mimic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdb15523-38f8-4abf-aed2-c47b269732ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pairs(df, image_dir=\"mimic/\", img_path_column=\"img_path\", report_column=\"report\"):\n",
    "    \"\"\"Load image-text pairs from MIMIC/Chexpert dataset using dataframe\"\"\"\n",
    "    valid_pairs = []\n",
    "    \n",
    "    # Iterate through dataframe rows\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            # Construct full image path\n",
    "            img_path = image_dir + row[img_path_column]\n",
    "            \n",
    "            # Load and preprocess image\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            image = image.resize((224, 224), Image.BICUBIC)\n",
    "            \n",
    "            # Get corresponding report text\n",
    "            text = row[report_column].strip()\n",
    "            \n",
    "            valid_pairs.append((image, text))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row[img_path_column]}: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "    return valid_pairs\n",
    "\n",
    "# Load MIMIC test set\n",
    "import pandas as pd\n",
    "mimic = pd.read_csv(\"mimic/mimic_cxr_labels_reports_splits.csv\", sep=\",\")\n",
    "mimic = mimic[mimic['split'] == \"test\"]\n",
    "mimic['img_path'] = mimic['img_path'].apply(lambda x: x.replace(\"data/MIMIC-CXR/mimic-cxr-jpg-2.0.0-small/images/\", \"\"))\n",
    "\n",
    "chexpert = pd.read_csv(\"chexpert/chexpert_plus_labels_with_5x200.csv\", sep=\"\\t\")\n",
    "chexpert = chexpert[chexpert['split'].isin([\"test\", \"valid\"])][['path_to_image', 'report']]\n",
    "\n",
    "# Get image-text pairs\n",
    "mimic_image_text_pairs = load_pairs(mimic, image_dir=\"mimic/\", img_path_column=\"img_path\", report_column=\"report\")\n",
    "chexpert_image_text_pairs = load_pairs(chexpert, image_dir=\"chexpert/\", img_path_column=\"path_to_image\", report_column=\"report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f95faf2-77b2-42c7-8445-d4b98638711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_metrics(image_features, text_features, logit_scale, logit_bias=None):\n",
    "    metrics = {}\n",
    "    logits_per_image = (logit_scale * image_features @ text_features.t())\n",
    "    if logit_bias:\n",
    "        logits_per_image += logit_bias\n",
    "    logits_per_image = logits_per_image.detach().cpu()\n",
    "    logits_per_text = logits_per_image.t().detach().cpu()\n",
    "\n",
    "    logits = {\"image_to_text\": logits_per_image, \"text_to_image\": logits_per_text}\n",
    "    ground_truth = torch.arange(len(text_features)).view(-1, 1)\n",
    "\n",
    "    for name, logit in logits.items():\n",
    "        ranking = torch.argsort(logit, descending=True)\n",
    "        preds = torch.where(ranking == ground_truth)[1]\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        metrics[f\"{name}_mean_rank\"] = preds.mean() + 1\n",
    "        metrics[f\"{name}_median_rank\"] = np.floor(np.median(preds)) + 1\n",
    "        for k in [1, 5, 10]:\n",
    "            metrics[f\"{name}_R@{k}\"] = np.mean(preds < k)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def process_image_text_pairs(image_text_pairs, model, tokenizer, preprocess, device, batch_size=100, use_logit_bias=False):\n",
    "    \"\"\"\n",
    "    Process image-text pairs in batches and extract features using CLIP model\n",
    "    \n",
    "    Args:\n",
    "        image_text_pairs: List of (image, text) tuples\n",
    "        model: CLIP model\n",
    "        tokenizer: CLIP tokenizer\n",
    "        preprocess: Image preprocessing function\n",
    "        device: Device to run model on\n",
    "        batch_size: Batch size for processing\n",
    "        \n",
    "    Returns:\n",
    "        image_features: Tensor of image features\n",
    "        text_features: Tensor of text features\n",
    "        logit_scale: Model's logit scale\n",
    "    \"\"\"\n",
    "    n_pairs = len(image_text_pairs)\n",
    "    all_image_features = []\n",
    "    all_text_features = []\n",
    "    logit_scale = None\n",
    "\n",
    "    # Process pairs in batches\n",
    "    for i in tqdm(range(0, n_pairs, batch_size), desc=\"Processing batches\"):\n",
    "        batch_slice = slice(i, min(i + batch_size, n_pairs))\n",
    "        current_batch = image_text_pairs[batch_slice]\n",
    "        \n",
    "        # Prepare batch inputs\n",
    "        batch_images = [img for img, _ in current_batch]\n",
    "        batch_texts = [text for _, text in current_batch]\n",
    "        \n",
    "        images = torch.stack([preprocess(img) for img in batch_images]).to(device)\n",
    "        texts = tokenizer(batch_texts, context_length=512).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            #in case of siglip, use the get_logits function directly as it has a bias term\n",
    "            if use_logit_bias:\n",
    "                image_features, text_features, logit_scale, logit_bias = model(images, texts)\n",
    "            else:\n",
    "                image_features, text_features, logit_scale = model(images, texts) # enable this for clip\n",
    "                logit_bias=None\n",
    "            \n",
    "            all_image_features.append(image_features)\n",
    "            all_text_features.append(text_features)\n",
    "    \n",
    "    return torch.cat(all_image_features), torch.cat(all_text_features), logit_scale, logit_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1d7a00a-bb62-438c-abd5-e93b9130923f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b214dfdb30741caba2907d5957f14b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics:\n",
      "image_to_text_mean_rank: 150.6711\n",
      "image_to_text_median_rank: 21.0000\n",
      "image_to_text_R@1: 0.0961\n",
      "image_to_text_R@5: 0.2820\n",
      "image_to_text_R@10: 0.3861\n",
      "text_to_image_mean_rank: 155.6424\n",
      "text_to_image_median_rank: 21.0000\n",
      "text_to_image_R@1: 0.1016\n",
      "text_to_image_R@5: 0.2888\n",
      "text_to_image_R@10: 0.3945\n"
     ]
    }
   ],
   "source": [
    "# This cell is for siglip style inference.\n",
    "# for clip, do this:\n",
    "# image_features, text_features, logit_scale, logit_bias = process_image_text_pairs(mimic_image_text_pairs, model, tokenizer, preprocess, device, batch_size=500, use_logit_bias=False)\n",
    "# mimic_metrics = get_clip_metrics(image_features, text_features, logit_scale)\n",
    "\n",
    "# Concatenate all features\n",
    "image_features, text_features, logit_scale, logit_bias = process_image_text_pairs(\n",
    "    mimic_image_text_pairs, model, tokenizer, preprocess, device, batch_size=500, use_logit_bias=True)\n",
    "\n",
    "# Compute metrics on all data\n",
    "mimic_metrics = get_clip_metrics(image_features, text_features, logit_scale, logit_bias)\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nMetrics:\")\n",
    "for key, value in mimic_metrics.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4427829d-669f-4d16-b49c-dc870b11227b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4381926835a4ee39393047d85a6052f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics:\n",
      "image_to_text_mean_rank: 63.5640\n",
      "image_to_text_median_rank: 7.0000\n",
      "image_to_text_R@1: 0.2196\n",
      "image_to_text_R@5: 0.4579\n",
      "image_to_text_R@10: 0.5583\n",
      "text_to_image_mean_rank: 80.7196\n",
      "text_to_image_median_rank: 6.0000\n",
      "text_to_image_R@1: 0.2350\n",
      "text_to_image_R@5: 0.4708\n",
      "text_to_image_R@10: 0.5810\n"
     ]
    }
   ],
   "source": [
    "# This cell is for siglip style inference\n",
    "# for clip, do this:\n",
    "# image_features, text_features, logit_scale, logit_bias = process_image_text_pairs(mimic_image_text_pairs, model, tokenizer, preprocess, device, batch_size=500, use_logit_bias=False)\n",
    "# mimic_metrics = get_clip_metrics(image_features, text_features, logit_scale)\n",
    "\n",
    "# Concatenate all features\n",
    "image_features, text_features, logit_scale, logit_bias = process_image_text_pairs(\n",
    "    chexpert_image_text_pairs, model, tokenizer, preprocess, device, batch_size=500, use_logit_bias=True)\n",
    "\n",
    "# Compute metrics on all data\n",
    "chexpert_metrics = get_clip_metrics(image_features, text_features, logit_scale, logit_bias)\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nMetrics:\")\n",
    "for key, value in chexpert_metrics.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54a2dc18-825a-4a7d-81b3-309083bd22c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all():\n",
    "    # List of model definitions using each model's configuration and checkpoint\n",
    "    models_info = [\n",
    "        {\n",
    "            \"model_name\": \"scratch-transformer\",\n",
    "            \"json_file\": \"scratch-transformer-custom-vit.json\",\n",
    "            \"ckpt\": \"scratch-transformer-epoch_50.pt\"\n",
    "        },\n",
    "        {\n",
    "            \"model_name\": \"bert\",\n",
    "            \"json_file\": \"bert-custom-vit.json\",\n",
    "            \"ckpt\": \"bert-epoch_50.pt\"\n",
    "        },\n",
    "        {\n",
    "            \"model_name\": \"cxr-bert\",\n",
    "            \"json_file\": \"cxr-bert-custom-vit.json\",\n",
    "            \"ckpt\": \"cxr-bert-epoch_50.pt\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Iterate over each model definition, load the model, and evaluate on both datasets.\n",
    "    for info in models_info:\n",
    "        print(f\"\\n========== Evaluating model: {info['model_name']} ==========\")\n",
    "        model_key = f\"{info['model_name']}_local\"\n",
    "        with open(info[\"json_file\"], \"r\") as f:\n",
    "            config = json.load(f)\n",
    "        model_cfg = config[\"model_cfg\"]\n",
    "        preprocess_cfg = config[\"preprocess_cfg\"]\n",
    "        _MODEL_CONFIGS[model_key] = model_cfg\n",
    "        tokenizer = get_tokenizer(model_key)\n",
    "        model, _, preprocess = create_model_and_transforms(\n",
    "            model_name=model_key,\n",
    "            pretrained=info[\"ckpt\"],\n",
    "            **{f\"image_{k}\": v for k, v in preprocess_cfg.items()},\n",
    "        )\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        # Evaluate on MIMIC dataset\n",
    "        print(f\"\\nMetrics for {info['model_name']} on MIMIC dataset:\")\n",
    "        img_features, txt_features, scale = process_image_text_pairs(\n",
    "            mimic_image_text_pairs, model, tokenizer, preprocess, device, batch_size=100)\n",
    "        metrics = get_clip_metrics(img_features, txt_features, scale)\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "        # Evaluate on Chexpert dataset\n",
    "        print(f\"\\nMetrics for {info['model_name']} on Chexpert dataset:\")\n",
    "        img_features, txt_features, scale = process_image_text_pairs(\n",
    "            chexpert_image_text_pairs, model, tokenizer, preprocess, device, batch_size=100)\n",
    "        metrics = get_clip_metrics(img_features, txt_features, scale)\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c535724-823e-4556-90e2-cf42e54e2bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Evaluating model: scratch-transformer ==========\n",
      "\n",
      "Metrics for scratch-transformer on MIMIC dataset:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8395f1139c64d399ae1f4210295565d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_to_text_mean_rank: 201.0337\n",
      "image_to_text_median_rank: 26.0000\n",
      "image_to_text_Hit@1: 0.0861\n",
      "image_to_text_Hit@5: 0.2594\n",
      "image_to_text_Hit@10: 0.3561\n",
      "image_to_text_MRR: 0.1727\n",
      "text_to_image_mean_rank: 197.2134\n",
      "text_to_image_median_rank: 25.0000\n",
      "text_to_image_Hit@1: 0.0849\n",
      "text_to_image_Hit@5: 0.2654\n",
      "text_to_image_Hit@10: 0.3644\n",
      "text_to_image_MRR: 0.1750\n",
      "\n",
      "Metrics for scratch-transformer on Chexpert dataset:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7235f65a764899a2f83579ecdaae1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_to_text_mean_rank: 110.7950\n",
      "image_to_text_median_rank: 20.0000\n",
      "image_to_text_Hit@1: 0.1718\n",
      "image_to_text_Hit@5: 0.3541\n",
      "image_to_text_Hit@10: 0.4222\n",
      "image_to_text_MRR: 0.2629\n",
      "text_to_image_mean_rank: 98.4279\n",
      "text_to_image_median_rank: 11.0000\n",
      "text_to_image_Hit@1: 0.1985\n",
      "text_to_image_Hit@5: 0.3955\n",
      "text_to_image_Hit@10: 0.4838\n",
      "text_to_image_MRR: 0.2968\n",
      "\n",
      "========== Evaluating model: bert ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bda51fab8aa4d11a21be8d105fa22d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a21c604e174cd3bf028e0992cd72df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb271fd44594233af5b29db1d3306e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90b8d28a6694833bf4a5c790f82bf95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for bert on MIMIC dataset:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71151a5dc518455d96e2ad9ab9eb97bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_to_text_mean_rank: 157.9971\n",
      "image_to_text_median_rank: 26.0000\n",
      "image_to_text_Hit@1: 0.0816\n",
      "image_to_text_Hit@5: 0.2469\n",
      "image_to_text_Hit@10: 0.3460\n",
      "image_to_text_MRR: 0.1699\n",
      "text_to_image_mean_rank: 168.1653\n",
      "text_to_image_median_rank: 26.0000\n",
      "text_to_image_Hit@1: 0.0826\n",
      "text_to_image_Hit@5: 0.2452\n",
      "text_to_image_Hit@10: 0.3470\n",
      "text_to_image_MRR: 0.1690\n",
      "\n",
      "Metrics for bert on Chexpert dataset:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf238fff45e4565acacd6a33388a7ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_to_text_mean_rank: 63.8979\n",
      "image_to_text_median_rank: 7.0000\n",
      "image_to_text_Hit@1: 0.2472\n",
      "image_to_text_Hit@5: 0.4554\n",
      "image_to_text_Hit@10: 0.5478\n",
      "image_to_text_MRR: 0.3489\n",
      "text_to_image_mean_rank: 82.2585\n",
      "text_to_image_median_rank: 7.0000\n",
      "text_to_image_Hit@1: 0.2131\n",
      "text_to_image_Hit@5: 0.4627\n",
      "text_to_image_Hit@10: 0.5648\n",
      "text_to_image_MRR: 0.3312\n",
      "\n",
      "========== Evaluating model: cxr-bert ==========\n",
      "\n",
      "Metrics for cxr-bert on MIMIC dataset:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e55274d1c3e4e93a85606d3d7eb8902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_to_text_mean_rank: 143.9335\n",
      "image_to_text_median_rank: 22.0000\n",
      "image_to_text_Hit@1: 0.0950\n",
      "image_to_text_Hit@5: 0.2760\n",
      "image_to_text_Hit@10: 0.3819\n",
      "image_to_text_MRR: 0.1872\n",
      "text_to_image_mean_rank: 141.4169\n",
      "text_to_image_median_rank: 21.0000\n",
      "text_to_image_Hit@1: 0.0942\n",
      "text_to_image_Hit@5: 0.2694\n",
      "text_to_image_Hit@10: 0.3778\n",
      "text_to_image_MRR: 0.1866\n",
      "\n",
      "Metrics for cxr-bert on Chexpert dataset:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af317e4f99824d74aa21dd210628c48f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_to_text_mean_rank: 72.6442\n",
      "image_to_text_median_rank: 8.0000\n",
      "image_to_text_Hit@1: 0.2261\n",
      "image_to_text_Hit@5: 0.4352\n",
      "image_to_text_Hit@10: 0.5389\n",
      "image_to_text_MRR: 0.3287\n",
      "text_to_image_mean_rank: 84.6823\n",
      "text_to_image_median_rank: 7.0000\n",
      "text_to_image_Hit@1: 0.2212\n",
      "text_to_image_Hit@5: 0.4538\n",
      "text_to_image_Hit@10: 0.5527\n",
      "text_to_image_MRR: 0.3301\n"
     ]
    }
   ],
   "source": [
    "compute_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d28c070-9dfc-41ee-8499-5f740fccc4f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
