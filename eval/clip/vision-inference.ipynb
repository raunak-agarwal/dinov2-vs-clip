{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c873c6a4-454e-4661-970a-77109b3bb437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model code exactly as this https://github.com/mlfoundations/open_clip/blob/main/src/open_clip/timm_model.py\n",
    "# Config from here https://github.com/mlfoundations/open_clip/blob/b2f1403605aade5a004434076246b6bc741aa47d/src/open_clip/model.py#L27\n",
    "\n",
    "\n",
    "import logging\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import timm\n",
    "from timm.layers import Mlp, to_2tuple\n",
    "#from timm.layers import RotAttentionPool2d \n",
    "#from timm.layers import AttentionPool2d as AbsAttentionPool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b40c30d0-bd4d-4379-b284-80712c9e1640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embed_dim': 768,\n",
       " 'custom_text': True,\n",
       " 'vision_cfg': {'image_size': 224,\n",
       "  'timm_model_name': 'vit_base_patch16_224',\n",
       "  'timm_model_pretrained': False,\n",
       "  'timm_proj': 'mlp',\n",
       "  'timm_drop_path': 0.1},\n",
       " 'text_cfg': {'context_length': 512,\n",
       "  'vocab_size': 49408,\n",
       "  'width': 768,\n",
       "  'heads': 8,\n",
       "  'layers': 12}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"custom-vit3.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44a81b0a-e8ec-4d43-ac00-c2c84d556378",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPTimmModel(nn.Module):\n",
    "    \"\"\" timm model adapter\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_name,\n",
    "            embed_dim,\n",
    "            image_size=224,\n",
    "            pool='avg',\n",
    "            proj='linear',\n",
    "            proj_bias=False,\n",
    "            drop=0.,\n",
    "            drop_path=None,\n",
    "            patch_drop=None,\n",
    "            pretrained=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if timm is None:\n",
    "            raise RuntimeError(\"Please `pip install timm` to use timm models.\")\n",
    "        self.image_size = to_2tuple(image_size)\n",
    "\n",
    "        # setup kwargs that may not be common across all models\n",
    "        timm_kwargs = {}\n",
    "        if drop_path is not None:\n",
    "            timm_kwargs['drop_path_rate'] = drop_path\n",
    "        if patch_drop is not None:\n",
    "            timm_kwargs['patch_drop_rate'] = patch_drop\n",
    "\n",
    "        custom_pool = pool in ('abs_attn', 'rot_attn')\n",
    "        if proj:\n",
    "            assert proj in (\"linear\", \"mlp\", \"none\")\n",
    "        extra_proj = proj in (\"linear\", \"mlp\")\n",
    "        if not extra_proj and not custom_pool:\n",
    "            # use network classifier head as projection if no proj specified and no custom pooling used\n",
    "            # if projection is explicitly set to \"none\" will be pass through from network trunk\n",
    "            proj_dim = 0 if proj == 'none' else embed_dim\n",
    "            self.trunk = timm.create_model(\n",
    "                model_name,\n",
    "                num_classes=proj_dim,\n",
    "                global_pool=pool,\n",
    "                pretrained=pretrained,\n",
    "                **timm_kwargs,\n",
    "            )\n",
    "            prev_chs = embed_dim\n",
    "        else:\n",
    "            self.trunk = timm.create_model(\n",
    "                model_name,\n",
    "                pretrained=pretrained,\n",
    "                **timm_kwargs,\n",
    "            )\n",
    "            feat_size = self.trunk.default_cfg.get('pool_size', None)\n",
    "            feature_ndim = 1 if not feat_size else 2\n",
    "            if custom_pool:\n",
    "                assert feature_ndim == 2\n",
    "                # if attn pooling used, remove both classifier and default pool\n",
    "                self.trunk.reset_classifier(0, global_pool='')\n",
    "            else:\n",
    "                # reset global pool if pool config set, otherwise leave as network default\n",
    "                reset_kwargs = dict(global_pool=pool) if pool else {}\n",
    "                self.trunk.reset_classifier(0, **reset_kwargs)\n",
    "            prev_chs = self.trunk.num_features\n",
    "\n",
    "        head_layers = OrderedDict()\n",
    "\n",
    "        # # Add custom pooling to head\n",
    "        # if pool == 'abs_attn':\n",
    "        #     head_layers['pool'] = AbsAttentionPool2d(prev_chs, feat_size=feat_size, out_features=embed_dim)\n",
    "        #     prev_chs = embed_dim\n",
    "        # elif pool == 'rot_attn':\n",
    "        #     head_layers['pool'] = RotAttentionPool2d(prev_chs, out_features=embed_dim)\n",
    "\n",
    "        # NOTE attention pool ends with a projection layer, so proj should usually be set to '' if such pooling is used\n",
    "        if proj == 'linear':\n",
    "            head_layers['drop'] = nn.Dropout(drop)\n",
    "            head_layers['proj'] = nn.Linear(prev_chs, embed_dim, bias=proj_bias)\n",
    "        elif proj == 'mlp':\n",
    "            head_layers['mlp'] = Mlp(prev_chs, 2 * embed_dim, embed_dim, drop=(drop, 0), bias=(True, proj_bias))\n",
    "\n",
    "        self.head = nn.Sequential(head_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.trunk(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59796a48-4d6f-4877-bf2b-924e21752fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPTimmModel(model_name=config['vision_cfg']['timm_model_name'], \n",
    "                      embed_dim=config['embed_dim'], proj=config['vision_cfg']['timm_proj'],\n",
    "                      drop_path=config['vision_cfg']['timm_drop_path']\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd968c06-4fe2-4946-a7be-9bbb734dc7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimmModel(\n",
       "  (trunk): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (patch_drop): Identity()\n",
       "    (norm_pre): Identity()\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): DropPath(drop_prob=0.009)\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): DropPath(drop_prob=0.009)\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): DropPath(drop_prob=0.018)\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): DropPath(drop_prob=0.018)\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): DropPath(drop_prob=0.027)\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): DropPath(drop_prob=0.027)\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): DropPath(drop_prob=0.036)\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): DropPath(drop_prob=0.036)\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): DropPath(drop_prob=0.045)\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): DropPath(drop_prob=0.045)\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): DropPath(drop_prob=0.055)\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): DropPath(drop_prob=0.055)\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): DropPath(drop_prob=0.064)\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): DropPath(drop_prob=0.064)\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): DropPath(drop_prob=0.073)\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): DropPath(drop_prob=0.073)\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): DropPath(drop_prob=0.082)\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): DropPath(drop_prob=0.082)\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): DropPath(drop_prob=0.091)\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): DropPath(drop_prob=0.091)\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): DropPath(drop_prob=0.100)\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): DropPath(drop_prob=0.100)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (fc_norm): Identity()\n",
       "    (head_drop): Dropout(p=0.0, inplace=False)\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=1536, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=1536, out_features=768, bias=False)\n",
       "      (drop2): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fefb2a59-9ad7-4b1a-9ab2-6b2e3f44a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load(\"epoch_10.pt\", map_location=torch.device(\"cpu\"))\n",
    "state_dict = weights['state_dict']\n",
    "state_dict = {k.replace(\"visual.\", \"\") :v for k, v in state_dict.items() if \"visual\" in k}\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03e37231-e6de-45db-9c8d-d44b68212ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 768])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(10, 3, 224, 224)\n",
    "\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cf25c6f2-1ae6-4b53-ab5e-f272566f77bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9b9efbaf-df55-4092-8c65-a2b2f03426db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from open_clip.tokenizer import SimpleTokenizer\n",
    "\n",
    "def check_simple_tokenization(word):\n",
    "    tokenizer = SimpleTokenizer()\n",
    "\n",
    "    encoding = tokenizer.encode(word)\n",
    "    decoded_output = [tokenizer.decode([e]) for e in encoding]\n",
    "    decoded_output = [token.replace(\" \", \"\") for token in decoded_output if token]\n",
    "\n",
    "    print(f\"Word: {word}\")\n",
    "    print(f\"Decoded Text: {decoded_output}\")\n",
    "\n",
    "    return decoded_output\n",
    "\n",
    "def check_hf_tokenization(model_name: str, word: str):\n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Tokenize the word\n",
    "    tokenized_output = tokenizer.encode(word, add_special_tokens=True)\n",
    "    \n",
    "    # Decode the tokenized output\n",
    "    decoded_output = [tokenizer.decode(token, skip_special_tokens=True) for token in tokenized_output]\n",
    "    decoded_output = [token for token in decoded_output if token]\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Word: {word}\")\n",
    "    print(f\"Decoded Text: {decoded_output}\")\n",
    "\n",
    "    return decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "383ff848-4f50-4ab6-a83e-12779a9ca4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: Pneumomediastinum\n",
      "Decoded Text: ['pneu', 'mom', 'edi', 'ast', 'in', 'um']\n",
      "Model: microsoft/BiomedVLP-CXR-BERT-general\n",
      "Word: Pneumomediastinum\n",
      "Decoded Text: ['pneumomediastinum']\n",
      "Model: google-bert/bert-base-uncased\n",
      "Word: Pneumomediastinum\n",
      "Decoded Text: ['p', '##ne', '##um', '##ome', '##dia', '##sti', '##num']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['p', '##ne', '##um', '##ome', '##dia', '##sti', '##num']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Pneumomediastinum\"\n",
    "\n",
    "check_simple_tokenization(text)\n",
    "\n",
    "model_name = \"microsoft/BiomedVLP-CXR-BERT-general\"\n",
    "check_hf_tokenization(model_name, text)\n",
    "\n",
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "check_hf_tokenization(model_name, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "81a15162-0d5a-4d5f-a61a-21ff6d9b6908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: Cardiomediastinum\n",
      "Decoded Text: ['cardi', 'ome', 'di', 'ast', 'in', 'um']\n",
      "Model: microsoft/BiomedVLP-CXR-BERT-general\n",
      "Word: Cardiomediastinum\n",
      "Decoded Text: ['cardiomedias', '##tinum']\n",
      "Model: bert-base-uncased\n",
      "Word: Cardiomediastinum\n",
      "Decoded Text: ['card', '##iom', '##ed', '##ias', '##tin', '##um']\n",
      "Word: Cardiomegaly\n",
      "Decoded Text: ['cardi', 'ome', 'gal', 'y']\n",
      "Model: microsoft/BiomedVLP-CXR-BERT-general\n",
      "Word: Cardiomegaly\n",
      "Decoded Text: ['cardiomegaly']\n",
      "Model: bert-base-uncased\n",
      "Word: Cardiomegaly\n",
      "Decoded Text: ['card', '##iom', '##ega', '##ly']\n",
      "Word: Lung Lesion\n",
      "Decoded Text: ['lung', 'le', 'sion']\n",
      "Model: microsoft/BiomedVLP-CXR-BERT-general\n",
      "Word: Lung Lesion\n",
      "Decoded Text: ['lung', 'lesion']\n",
      "Model: bert-base-uncased\n",
      "Word: Lung Lesion\n",
      "Decoded Text: ['lung', 'les', '##ion']\n",
      "Word: Lung Opacity\n",
      "Decoded Text: ['lung', 'op', 'acity']\n",
      "Model: microsoft/BiomedVLP-CXR-BERT-general\n",
      "Word: Lung Opacity\n",
      "Decoded Text: ['lung', 'opacity']\n",
      "Model: bert-base-uncased\n",
      "Word: Lung Opacity\n",
      "Decoded Text: ['lung', 'op', '##ac', '##ity']\n",
      "Word: Edema\n",
      "Decoded Text: ['ede', 'ma']\n",
      "Model: microsoft/BiomedVLP-CXR-BERT-general\n",
      "Word: Edema\n",
      "Decoded Text: ['edema']\n",
      "Model: bert-base-uncased\n",
      "Word: Edema\n",
      "Decoded Text: ['ed', '##ema']\n",
      "Word: Consolidation\n",
      "Decoded Text: ['consolidation']\n",
      "Model: microsoft/BiomedVLP-CXR-BERT-general\n",
      "Word: Consolidation\n",
      "Decoded Text: ['consolidation']\n",
      "Model: bert-base-uncased\n",
      "Word: Consolidation\n",
      "Decoded Text: ['consolidation']\n",
      "Word: Pneumonia\n",
      "Decoded Text: ['pneumonia']\n",
      "Model: microsoft/BiomedVLP-CXR-BERT-general\n",
      "Word: Pneumonia\n",
      "Decoded Text: ['pneumonia']\n",
      "Model: bert-base-uncased\n",
      "Word: Pneumonia\n",
      "Decoded Text: ['pneumonia']\n",
      "Word: Atelectasis\n",
      "Decoded Text: ['ate', 'lec', 'tas', 'is']\n",
      "Model: microsoft/BiomedVLP-CXR-BERT-general\n",
      "Word: Atelectasis\n",
      "Decoded Text: ['atelectasis']\n",
      "Model: bert-base-uncased\n",
      "Word: Atelectasis\n",
      "Decoded Text: ['ate', '##le', '##cta', '##sis']\n",
      "Word: Pneumothorax\n",
      "Decoded Text: ['pneu', 'mo', 'thor', 'ax']\n",
      "Model: microsoft/BiomedVLP-CXR-BERT-general\n",
      "Word: Pneumothorax\n",
      "Decoded Text: ['pneumothorax']\n",
      "Model: bert-base-uncased\n",
      "Word: Pneumothorax\n",
      "Decoded Text: ['p', '##ne', '##um', '##otho', '##ra', '##x']\n",
      "Word: Pleural Effusion\n",
      "Decoded Text: ['ple', 'ural', 'ef', 'fusion']\n",
      "Model: microsoft/BiomedVLP-CXR-BERT-general\n",
      "Word: Pleural Effusion\n",
      "Decoded Text: ['pleural', 'effusion']\n",
      "Model: bert-base-uncased\n",
      "Word: Pleural Effusion\n",
      "Decoded Text: ['pl', '##eur', '##al', 'e', '##ff', '##usion']\n",
      "Word: Pleural Other\n",
      "Decoded Text: ['ple', 'ural', 'other']\n",
      "Model: microsoft/BiomedVLP-CXR-BERT-general\n",
      "Word: Pleural Other\n",
      "Decoded Text: ['pleural', 'other']\n",
      "Model: bert-base-uncased\n",
      "Word: Pleural Other\n",
      "Decoded Text: ['pl', '##eur', '##al', 'other']\n",
      "Word: Fracture\n",
      "Decoded Text: ['fracture']\n",
      "Model: microsoft/BiomedVLP-CXR-BERT-general\n",
      "Word: Fracture\n",
      "Decoded Text: ['fracture']\n",
      "Model: bert-base-uncased\n",
      "Word: Fracture\n",
      "Decoded Text: ['fracture']\n",
      "Word: Support Devices\n",
      "Decoded Text: ['support', 'devices']\n",
      "Model: microsoft/BiomedVLP-CXR-BERT-general\n",
      "Word: Support Devices\n",
      "Decoded Text: ['support', 'devices']\n",
      "Model: bert-base-uncased\n",
      "Word: Support Devices\n",
      "Decoded Text: ['support', 'devices']\n",
      "Word: Emphysema\n",
      "Decoded Text: ['em', 'phy', 'sema']\n",
      "Model: microsoft/BiomedVLP-CXR-BERT-general\n",
      "Word: Emphysema\n",
      "Decoded Text: ['emphysema']\n",
      "Model: bert-base-uncased\n",
      "Word: Emphysema\n",
      "Decoded Text: ['em', '##phy', '##se', '##ma']\n",
      "Word: Fibrosis\n",
      "Decoded Text: ['fibrosis']\n",
      "Model: microsoft/BiomedVLP-CXR-BERT-general\n",
      "Word: Fibrosis\n",
      "Decoded Text: ['fibrosis']\n",
      "Model: bert-base-uncased\n",
      "Word: Fibrosis\n",
      "Decoded Text: ['fi', '##bro', '##sis']\n",
      "Word: Hernia\n",
      "Decoded Text: ['her', 'nia']\n",
      "Model: microsoft/BiomedVLP-CXR-BERT-general\n",
      "Word: Hernia\n",
      "Decoded Text: ['hernia']\n",
      "Model: bert-base-uncased\n",
      "Word: Hernia\n",
      "Decoded Text: ['her', '##nia']\n",
      "Word: Infiltration\n",
      "Decoded Text: ['infiltr', 'ation']\n",
      "Model: microsoft/BiomedVLP-CXR-BERT-general\n",
      "Word: Infiltration\n",
      "Decoded Text: ['infiltration']\n",
      "Model: bert-base-uncased\n",
      "Word: Infiltration\n",
      "Decoded Text: ['in', '##filtration']\n",
      "Word: Mass\n",
      "Decoded Text: ['mass']\n",
      "Model: microsoft/BiomedVLP-CXR-BERT-general\n",
      "Word: Mass\n",
      "Decoded Text: ['mass']\n",
      "Model: bert-base-uncased\n",
      "Word: Mass\n",
      "Decoded Text: ['mass']\n",
      "Word: Nodule\n",
      "Decoded Text: ['no', 'du', 'le']\n",
      "Model: microsoft/BiomedVLP-CXR-BERT-general\n",
      "Word: Nodule\n",
      "Decoded Text: ['nodule']\n",
      "Model: bert-base-uncased\n",
      "Word: Nodule\n",
      "Decoded Text: ['nod', '##ule']\n",
      "Word: Pleural Thickening\n",
      "Decoded Text: ['ple', 'ural', 'thick', 'ening']\n",
      "Model: microsoft/BiomedVLP-CXR-BERT-general\n",
      "Word: Pleural Thickening\n",
      "Decoded Text: ['pleural', 'thickening']\n",
      "Model: bert-base-uncased\n",
      "Word: Pleural Thickening\n",
      "Decoded Text: ['pl', '##eur', '##al', 'thick', '##ening']\n",
      "Word: Pneumoperitoneum\n",
      "Decoded Text: ['pneu', 'mo', 'per', 'it', 'one', 'um']\n",
      "Model: microsoft/BiomedVLP-CXR-BERT-general\n",
      "Word: Pneumoperitoneum\n",
      "Decoded Text: ['pneumoperitoneum']\n",
      "Model: bert-base-uncased\n",
      "Word: Pneumoperitoneum\n",
      "Decoded Text: ['p', '##ne', '##um', '##oper', '##ito', '##ne', '##um']\n",
      "Word: Pneumomediastinum\n",
      "Decoded Text: ['pneu', 'mom', 'edi', 'ast', 'in', 'um']\n",
      "Model: microsoft/BiomedVLP-CXR-BERT-general\n",
      "Word: Pneumomediastinum\n",
      "Decoded Text: ['pneumomediastinum']\n",
      "Model: bert-base-uncased\n",
      "Word: Pneumomediastinum\n",
      "Decoded Text: ['p', '##ne', '##um', '##ome', '##dia', '##sti', '##num']\n",
      "Word: Subcutaneous Emphysema\n",
      "Decoded Text: ['sub', 'cu', 'taneous', 'em', 'phy', 'sema']\n",
      "Model: microsoft/BiomedVLP-CXR-BERT-general\n",
      "Word: Subcutaneous Emphysema\n",
      "Decoded Text: ['subcutaneous', 'emphysema']\n",
      "Model: bert-base-uncased\n",
      "Word: Subcutaneous Emphysema\n",
      "Decoded Text: ['sub', '##cut', '##aneous', 'em', '##phy', '##se', '##ma']\n",
      "Word: Tortuous Aorta\n",
      "Decoded Text: ['tor', 'tuous', 'aor', 'ta']\n",
      "Model: microsoft/BiomedVLP-CXR-BERT-general\n",
      "Word: Tortuous Aorta\n",
      "Decoded Text: ['tortuous', 'aorta']\n",
      "Model: bert-base-uncased\n",
      "Word: Tortuous Aorta\n",
      "Decoded Text: ['tor', '##tu', '##ous', 'ao', '##rta']\n",
      "Word: Calcification of the Aorta\n",
      "Decoded Text: ['calci', 'fication', 'of', 'the', 'aor', 'ta']\n",
      "Model: microsoft/BiomedVLP-CXR-BERT-general\n",
      "Word: Calcification of the Aorta\n",
      "Decoded Text: ['calcification', 'of', 'the', 'aorta']\n",
      "Model: bert-base-uncased\n",
      "Word: Calcification of the Aorta\n",
      "Decoded Text: ['cal', '##ci', '##fication', 'of', 'the', 'ao', '##rta']\n",
      "\n",
      "\\begin{table}[h!]\n",
      "\\centering\n",
      "\\begin{adjustbox}{width=1.2\\textwidth,center=1.1\\textwidth}\n",
      "\\begin{tabular}{|l|l|l|l|}\n",
      "\\hline\n",
      "\\textbf{Term} & \\textbf{CLIP Tokenizer} & \\textbf{CXR-BERT} & \\textbf{BERT} \\\\\n",
      "\\hline\n",
      "Cardiomediastinum & cardi-ome-di-ast-in-um & cardiomedias-tinum & card-iom-ed-ias-tin-um \\\\\n",
      "Cardiomegaly & cardi-ome-gal-y & cardiomegaly & card-iom-ega-ly \\\\\n",
      "Lung Lesion & lung-le-sion & lung-lesion & lung-les-ion \\\\\n",
      "Lung Opacity & lung-op-acity & lung-opacity & lung-op-ac-ity \\\\\n",
      "Edema & ede-ma & edema & ed-ema \\\\\n",
      "Consolidation & consolidation & consolidation & consolidation \\\\\n",
      "Pneumonia & pneumonia & pneumonia & pneumonia \\\\\n",
      "Atelectasis & ate-lec-tas-is & atelectasis & ate-le-cta-sis \\\\\n",
      "Pneumothorax & pneu-mo-thor-ax & pneumothorax & p-ne-um-otho-ra-x \\\\\n",
      "Pleural Effusion & ple-ural-ef-fusion & pleural-effusion & pl-eur-al-e-ff-usion \\\\\n",
      "Pleural Other & ple-ural-other & pleural-other & pl-eur-al-other \\\\\n",
      "Fracture & fracture & fracture & fracture \\\\\n",
      "Support Devices & support-devices & support-devices & support-devices \\\\\n",
      "Emphysema & em-phy-sema & emphysema & em-phy-se-ma \\\\\n",
      "Fibrosis & fibrosis & fibrosis & fi-bro-sis \\\\\n",
      "Hernia & her-nia & hernia & her-nia \\\\\n",
      "Infiltration & infiltr-ation & infiltration & in-filtration \\\\\n",
      "Mass & mass & mass & mass \\\\\n",
      "Nodule & no-du-le & nodule & nod-ule \\\\\n",
      "Pleural Thickening & ple-ural-thick-ening & pleural-thickening & pl-eur-al-thick-ening \\\\\n",
      "Pneumoperitoneum & pneu-mo-per-it-one-um & pneumoperitoneum & p-ne-um-oper-ito-ne-um \\\\\n",
      "Pneumomediastinum & pneu-mom-edi-ast-in-um & pneumomediastinum & p-ne-um-ome-dia-sti-num \\\\\n",
      "Subcutaneous Emphysema & sub-cu-taneous-em-phy-sema & subcutaneous-emphysema & sub-cut-aneous-em-phy-se-ma \\\\\n",
      "Tortuous Aorta & tor-tuous-aor-ta & tortuous-aorta & tor-tu-ous-ao-rta \\\\\n",
      "Calcification of the Aorta & calci-fication-of-the-aor-ta & calcification-of-the-aorta & cal-ci-fication-of-the-ao-rta \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\end{adjustbox}\n",
      "\\caption{Tokenization Comparison for various CXR-related terms}\n",
      "\\label{tab:tokenization_results}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "medical_terms = [\n",
    "    \"Cardiomediastinum\", \"Cardiomegaly\", \"Lung Lesion\",\n",
    "    \"Lung Opacity\", \"Edema\", \"Consolidation\", \"Pneumonia\", \"Atelectasis\",\n",
    "    \"Pneumothorax\", \"Pleural Effusion\", \"Pleural Other\", \"Fracture\",\n",
    "    \"Support Devices\", \"Emphysema\", \"Fibrosis\", \"Hernia\", \"Infiltration\",\n",
    "    \"Mass\", \"Nodule\", \"Pleural Thickening\", \"Pneumoperitoneum\",\n",
    "    \"Pneumomediastinum\", \"Subcutaneous Emphysema\", \"Tortuous Aorta\",\n",
    "    \"Calcification of the Aorta\"\n",
    "]\n",
    "\n",
    "latex_rows = []\n",
    "\n",
    "for term in medical_terms:\n",
    "    simple_tokens = check_simple_tokenization(term)\n",
    "    hf_tokens_1 = check_hf_tokenization(hf_model_1, term)\n",
    "    hf_tokens_2 = check_hf_tokenization(hf_model_2, term)\n",
    "\n",
    "    # Join tokens with hyphens instead of spaces\n",
    "    row = f\"{term} & {'-'.join(simple_tokens)} & {'-'.join(hf_tokens_1)} & {'-'.join(hf_tokens_2)} \\\\\\\\\"\n",
    "    latex_rows.append(row)\n",
    "\n",
    "# Construct the full LaTeX table\n",
    "latex_table = r\"\"\"\n",
    "\\begin{table}[h!]\n",
    "\\centering\n",
    "\\begin{adjustbox}{width=1.2\\textwidth,center=1.1\\textwidth}\n",
    "\\begin{tabular}{|l|l|l|l|}\n",
    "\\hline\n",
    "\\textbf{Term} & \\textbf{CLIP Tokenizer} & \\textbf{CXR-BERT} & \\textbf{BERT} \\\\\n",
    "\\hline\n",
    "\"\"\"\n",
    "latex_table += \"\\n\".join(latex_rows)\n",
    "latex_table += r\"\"\"\n",
    "\\hline\n",
    "\\end{tabular}\n",
    "\\end{adjustbox}\n",
    "\\caption{Tokenization Comparison for various CXR-related terms}\n",
    "\\label{tab:tokenization_results}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "\n",
    "print(latex_table.replace(\"#\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9861ae28-5061-4b49-bc2a-913bc98044d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
